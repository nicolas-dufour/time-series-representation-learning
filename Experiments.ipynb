{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Unsupervised Scalable Representation Learning for Multivariate Time Series**\n",
    "\n",
    "**Time Series Learning Project**\n",
    "\n",
    "This notebook implements some of the experiment we did to better understand the main idea of the paper. \n",
    "\n",
    "Contains:\n",
    "\n",
    "* Analysis of the model on Univariate time series (DodgerLoopDay dataset)\n",
    "* Analysis of the model on Multivariate time series (BasicMotions dataset)\n",
    "\n",
    "With each time some exploratory experiments.\n",
    "\n",
    " The data folder contains the data such that there are: `./data/DodgerLoopDay/DodgerLoopDay_TEST.tsv` and `./data/DodgerLoopDay/DodgerLoopDay_TRAIN.tsv`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "from datamodule import TimeSeriesDataModule\n",
    "from loss import TripletLoss\n",
    "from model import (\n",
    "    CausalCNN,\n",
    "    CausalCNNEncoder,\n",
    "    CausalConvolutionBlock,\n",
    "    Chomp1d,\n",
    "    SqueezeChannels,\n",
    ")\n",
    "from train import TimeSeriesEmbedder\n",
    "from utils import load_UCR_dataset\n",
    "\n",
    "root_data = \"./data/\"\n",
    "\n",
    "# The data folder contains the data such that there are:\n",
    "# ./data/DodgerLoopDay/DodgerLoopDay_TEST.tsv\n",
    "# ./data/DodgerLoopDay/DodgerLoopDay_TRAIN.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Univariate time series - BasicMotions\n",
    "\n",
    "This section aims at studying the article in the context of univariate time series. As an exploratory example, we used the dataset **DodgerLoopDay **:\n",
    "    \n",
    "        \"The traffic data are collected with the loop sensor installed on ramp for the 101 North freeway in Los Angeles. This location is close to Dodgers Stadium; therefore the traffic is affected by volume of visitors to the stadium. Missing values are represented with NaN. The classes are days of the week. - Class 1: Sunday - Class 2: Monday - Class 3: Tuesday - Class 4: Wednesday - Class 5: Thursday - Class 6: Friday - Class 7: Saturday.\"\n",
    "\n",
    "This section contains:\n",
    "* Some experiments on our model, dataloader, and loss, on this univariate dataset.\n",
    "\n",
    "![image.png](imgs/DodgerLoopDay_img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importation of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (78, 1, 288)\n",
      "y_train: (78,)\n",
      "X_test: (80, 1, 288)\n",
      "y_test: (80,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_UCR_dataset(root_data, \"DodgerLoopDay\")\n",
    "\n",
    "print(\"X_train: {}\".format(X_train.shape))\n",
    "print(\"y_train: {}\".format(y_train.shape))\n",
    "print(\"X_test: {}\".format(X_test.shape))\n",
    "print(\"y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters\n",
    "batch_size = 20\n",
    "num_workers = 2\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 1e-2\n",
    "lr = 0.001\n",
    "\n",
    "# Model parameter\n",
    "in_channels = 1\n",
    "channels = 40\n",
    "depth = 4\n",
    "reduced_size = 160\n",
    "out_channels = 320\n",
    "kernel_size = 3\n",
    "N_sample = 288\n",
    "\n",
    "# Data parameters\n",
    "train_path = os.path.join(root_data, \"FordA\", \"FordA_TRAIN.tsv\")\n",
    "val_path = os.path.join(root_data, \"FordA\", \"FordA_TEST.tsv\")\n",
    "\n",
    "\n",
    "# Datamodule importation\n",
    "datamodule = TimeSeriesDataModule(\n",
    "    train_path,\n",
    "    val_path,\n",
    "    batch_size,\n",
    "    num_workers,\n",
    "    min_length=20,\n",
    "    multivariate=False,\n",
    "    fill_na=True,\n",
    ")\n",
    "datamodule.setup()\n",
    "# Model definition\n",
    "model = TimeSeriesEmbedder(\n",
    "    in_channels=in_channels,\n",
    "    channels=channels,\n",
    "    depth=depth,\n",
    "    reduced_size=reduced_size,\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    betas=betas,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "max_steps = 2000\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    mode=\"min\",\n",
    "    monitor=\"train_loss_epoch\",\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"causalcnn-{epoch:02d}-{train_loss_epoch:.2f}\",\n",
    "    period=len(datamodule.train_dataloader()),\n",
    ")\n",
    "\n",
    "wandb_logger = pl.loggers.WandbLogger(\n",
    "    project=\"Self Supervised Time Series LEarning\", name=\"Run n 1\"\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=max_steps,\n",
    "    logger=wandb_logger,\n",
    "    val_check_interval=50,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicolas-dufour\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.23 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">Run n 1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nicolas-dufour/Self%20Supervised%20Time%20Series%20LEarning\" target=\"_blank\">https://wandb.ai/nicolas-dufour/Self%20Supervised%20Time%20Series%20LEarning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nicolas-dufour/Self%20Supervised%20Time%20Series%20LEarning/runs/k6z6zjby\" target=\"_blank\">https://wandb.ai/nicolas-dufour/Self%20Supervised%20Time%20Series%20LEarning/runs/k6z6zjby</a><br/>\n",
       "                Run data is saved locally in <code>/mnt/c/Users/nicod/Documents/MVA/Time Series/time-series-representation-learning/wandb/run-20210330_000112-k6z6zjby</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | CausalCNNEncoder | 189 K \n",
      "1 | criterium | TripletLoss      | 0     \n",
      "-----------------------------------------------\n",
      "189 K     Trainable params\n",
      "0         Non-trainable params\n",
      "189 K     Total params\n",
      "0.757     Total estimated model params size (MB)\n",
      "/home/nicolas-dufour/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas-dufour/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3fd0353ca045acb9cd7f1bad7e8e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfa776e627d4aeebebda9cd8564c7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate time series - BasicMotions\n",
    "\n",
    "This section aims at studying the article in the context of multivariate time series. As an exploratory example, we used the dataset **Basic Motions**:\n",
    "    \n",
    "        \"The data was generated as part of a student project where four students performed four activities whilst wearing a smart watch. The watch collects 3D accelerometer and a 3D gyroscope It consists of four classes, which are walking, resting, running and badminton. Participants were required to record motion a total of five times, and the data is sampled once every tenth of a second, for a ten second period.\"\n",
    "\n",
    "This section contains:\n",
    "* A visualization of the dataset\n",
    "* Some experiments on the model provided by the author, on this multivariate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Importation of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-939827b1c207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_basic_motions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muea_dataset_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_basic_motions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyts'"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "from pyts.datasets import load_basic_motions, uea_dataset_list\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_basic_motions(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing: normalization\n",
    "X_train = (X_train - X_train.mean(axis=2)[:, :, None]) / X_train.std(axis=2)[:, :, None]\n",
    "X_test = (X_test - X_test.mean(axis=2)[:, :, None]) / X_test.std(axis=2)[:, :, None]\n",
    "\n",
    "\n",
    "X_train = torch.from_numpy(X_train).double()\n",
    "X_test = torch.from_numpy(X_test).double()\n",
    "if torch.cuda.is_available():\n",
    "    X_train = X_train.cuda()\n",
    "    X_test = X_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Visualization\n",
    "\n",
    "    The data was generated as part of a student project where four students performed four activities whilst wearing a smart watch. The watch collects 3D accelerometer and a 3D gyroscope It consists of four classes, which are walking, resting, running and badminton. Participants were required to record motion a total of five times, and the data is sampled once every tenth of a second, for a ten second period.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"Accelerometer - X\",\n",
    "    \"Accelerometer - Y\",\n",
    "    \"Accelerometer - Z\",\n",
    "    \"Gyroscope - Yaw\",\n",
    "    \"Gyroscope - Pitch\",\n",
    "    \"Gyroscope - Roll\",\n",
    "]\n",
    "if True:\n",
    "    index = 0\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "    axes = []\n",
    "    for i in range(6):\n",
    "        axes.append(fig.add_subplot(6, 2, 2 * i + 1))\n",
    "        axes[i].set_title(\n",
    "            \"{}\".format(str(y_train[index])[2:-1]), weight=\"bold\", fontsize=18\n",
    "        )\n",
    "        axes[i].plot(X_train[index, i, :], label=labels[i])\n",
    "        axes[i].legend(loc=1)\n",
    "    index = -1\n",
    "    for i in range(6):\n",
    "        axes.append(fig.add_subplot(6, 2, 2 * (i + 1)))\n",
    "        axes[6 + i].set_title(\n",
    "            \"{}\".format(str(y_train[index])[2:-1]), weight=\"bold\", fontsize=18\n",
    "        )\n",
    "        axes[6 + i].plot(X_train[index, i, :], label=labels[i])\n",
    "        axes[6 + i].legend(loc=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Causal CNN Encoder - Exploration\n",
    "\n",
    "This section runs different experiment to investigate the role of the different module of the architecture:\n",
    "\n",
    "* The global average pooling layer to squeeze the temporal dimension, which is supposed to regularize the model compared to using a fully-connected layer.\n",
    "* The different hyper-parameters of the model:\n",
    "        * The depth of the network (i.e the number of causal CNN blocks, e.g 10 by default)\n",
    "        * The number of channels (e.g 40 by default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Global Average Pooling as a Regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_channels = 6\n",
    "channels = 40\n",
    "depth = 4\n",
    "reduced_size = 160\n",
    "out_channels = 320\n",
    "kernel_size = 3\n",
    "N_sample = 100\n",
    "\n",
    "# The whole model\n",
    "causalEncoder = CausalCNNEncoder(\n",
    "    in_channels=in_channels,\n",
    "    channels=channels,\n",
    "    depth=depth,\n",
    "    reduced_size=reduced_size,\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=kernel_size,\n",
    ").double()\n",
    "\n",
    "# The whole model without the last global average pooling and FC between reduced_size and out_channel\n",
    "causal_cnn = CausalCNN(\n",
    "    in_channels=in_channels,\n",
    "    channels=channels,\n",
    "    depth=depth,\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=kernel_size,\n",
    ").double()\n",
    "\n",
    "\n",
    "##### BUILDING EACH BLOCK OF THE MODEL\n",
    "\n",
    "# Each of the Causal CNN Blocks (of width 4)\n",
    "ConvBlock_1 = CausalConvolutionBlock(\n",
    "    in_channels=in_channels, out_channels=channels, dilation=1, kernel_size=3\n",
    ").double()\n",
    "ConvBlock_2 = CausalConvolutionBlock(\n",
    "    in_channels=channels, out_channels=channels, dilation=2, kernel_size=3\n",
    ").double()\n",
    "ConvBlock_3 = CausalConvolutionBlock(\n",
    "    in_channels=channels, out_channels=channels, dilation=4, kernel_size=3\n",
    ").double()\n",
    "ConvBlock_4 = CausalConvolutionBlock(\n",
    "    in_channels=channels, out_channels=reduced_size, dilation=8, kernel_size=3\n",
    ").double()\n",
    "\n",
    "# Global average pooling\n",
    "reduce_size = torch.nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "# Squeez the last (third) temporal dimension\n",
    "squeeze = SqueezeChannels()\n",
    "\n",
    "# last fully connected layer to go from reduced_size to out_channel\n",
    "linear = torch.nn.Linear(reduced_size, out_channels).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Data shape flow using the global average pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_from_scratch = [\n",
    "    ConvBlock_1,\n",
    "    ConvBlock_2,\n",
    "    ConvBlock_3,\n",
    "    ConvBlock_4,\n",
    "    reduce_size,\n",
    "    squeeze,\n",
    "    linear,\n",
    "]\n",
    "model_name_from_scratch = [\n",
    "    \"ConvBlock_1\",\n",
    "    \"ConvBlock_2\",\n",
    "    \"ConvBlock_3\",\n",
    "    \"ConvBlock_4\",\n",
    "    \"Global Average Pooling\",\n",
    "    \"squeezing\",\n",
    "    \"final FC\",\n",
    "]\n",
    "\n",
    "print(\"Input Shape:\")\n",
    "print(list(X_train.shape), \"\\n\")\n",
    "input = X_train\n",
    "for block_ii in range(len(model_from_scratch)):\n",
    "    print(\"{}:\".format(model_name_from_scratch[block_ii]))\n",
    "    output = model_from_scratch[block_ii](input)\n",
    "    print(list(output.shape), \"\\n\")\n",
    "    input = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Data shape flow using the a fully-connected layer to squeeze the temporal dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instead of the Global Average Pooling\n",
    "linear_to_squeeze = torch.nn.Linear(N_sample, 1).double()\n",
    "\n",
    "\n",
    "model_from_scratch_experiment = [\n",
    "    ConvBlock_1,\n",
    "    ConvBlock_2,\n",
    "    ConvBlock_3,\n",
    "    ConvBlock_4,\n",
    "    linear_to_squeeze,\n",
    "    squeeze,\n",
    "    linear,\n",
    "]\n",
    "model_name_from_scratch_experiment = [\n",
    "    \"ConvBlock_1\",\n",
    "    \"ConvBlock_2\",\n",
    "    \"ConvBlock_3\",\n",
    "    \"ConvBlock_4\",\n",
    "    \"linear_to_squeeze\",\n",
    "    \"squeezing\",\n",
    "    \"final FC\",\n",
    "]\n",
    "\n",
    "print(\"Input Shape:\")\n",
    "print(list(X_train.shape), \"\\n\")\n",
    "input = X_train\n",
    "for block_ii in range(len(model_from_scratch_experiment)):\n",
    "    print(\"{}:\".format(model_name_from_scratch_experiment[block_ii]))\n",
    "    output = model_from_scratch_experiment[block_ii](input)\n",
    "    print(list(output.shape), \"\\n\")\n",
    "    input = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Comparison of the performances**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Analysis of the hyperparameters of the model\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
