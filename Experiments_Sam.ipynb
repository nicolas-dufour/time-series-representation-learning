{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Unsupervised Scalable Representation Learning for Multivariate Time Series**\n",
    "\n",
    "**Time Series Learning Project**\n",
    "\n",
    "This notebook implements some of the experiment we did to better understand the main idea of the paper. \n",
    "\n",
    "Contains:\n",
    "\n",
    "* Analysis of the model on Univariate time series (DodgerLoopDay dataset)\n",
    "* Analysis of the model on Multivariate time series (BasicMotions dataset)\n",
    "\n",
    "With each time some exploratory experiments.\n",
    "\n",
    " The data folder contains the data such that there are: `./data/DodgerLoopDay/DodgerLoopDay_TEST.tsv` and `./data/DodgerLoopDay/DodgerLoopDay_TRAIN.tsv`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from datamodule import TimeSeriesDataModule\n",
    "from model import (Chomp1d, SqueezeChannels, CausalConvolutionBlock, CausalCNN, CausalCNNEncoder)\n",
    "from train import TimeSeriesEmbedder\n",
    "from utils import (load_UCR_dataset)\n",
    "from loss import TripletLoss\n",
    "\n",
    "root_data = './data/'\n",
    "\n",
    "# The data folder contains the data such that there are:\n",
    "# ./data/DodgerLoopDay/DodgerLoopDay_TEST.tsv\n",
    "# ./data/DodgerLoopDay/DodgerLoopDay_TRAIN.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Univariate time series \n",
    "\n",
    "This section aims at studying the article in the context of univariate time series. As an exploratory example, we used the dataset **DodgerLoopDay **:\n",
    "    \n",
    "        \"The traffic data are collected with the loop sensor installed on ramp for the 101 North freeway in Los Angeles. This location is close to Dodgers Stadium; therefore the traffic is affected by volume of visitors to the stadium. Missing values are represented with NaN. The classes are days of the week. - Class 1: Sunday - Class 2: Monday - Class 3: Tuesday - Class 4: Wednesday - Class 5: Thursday - Class 6: Friday - Class 7: Saturday.\"\n",
    "\n",
    "This section contains:\n",
    "* Some experiments on our model, dataloader, and loss, on this univariate dataset.\n",
    "\n",
    "![image.png](imgs/DodgerLoopDay_img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training of the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters \n",
    "batch_size = 20\n",
    "num_workers = 4\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 1e-2\n",
    "lr = 0.001\n",
    "\n",
    "# Model parameter \n",
    "in_channels = 1\n",
    "channels = 40\n",
    "depth = 4\n",
    "reduced_size = 160\n",
    "out_channels = 320\n",
    "kernel_size = 3\n",
    "N_sample = 288\n",
    "\n",
    "# Data parameters\n",
    "train_path = os.path.join(root_data,'FordA','FordA_TRAIN.tsv')\n",
    "val_path = os.path.join(root_data,'FordA','FordA_TEST.tsv')\n",
    "\n",
    "train_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TRAIN.tsv')\n",
    "val_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TEST.tsv')\n",
    "\n",
    "\n",
    "# Datamodule importation\n",
    "datamodule = TimeSeriesDataModule(train_path, val_path, batch_size, num_workers, min_length=20, multivariate=False, fill_na=True)\n",
    "\n",
    "# Model definition\n",
    "model = TimeSeriesEmbedder(\n",
    "    in_channels = in_channels,\n",
    "    channels = channels,\n",
    "    depth = depth,\n",
    "    reduced_size = reduced_size,\n",
    "    out_channels = out_channels,\n",
    "    kernel_size = kernel_size,\n",
    "    lr=lr,\n",
    "    weight_decay = weight_decay,\n",
    "    betas = betas,\n",
    "    train_path=train_path,\n",
    "    test_path=val_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "     mode ='min',\n",
    "     monitor='train_loss_epoch',\n",
    "     dirpath='checkpoints',\n",
    "    filename='causalcnn-{epoch:02d}-{train_loss_epoch:.2f}'\n",
    ")\n",
    "\n",
    "wandb_logger = pl.loggers.WandbLogger(project='Self Supervised Time Series Learning', name='Test 1 sur le dataset DodgerLoopDay')\n",
    "trainer  = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    logger = wandb_logger,\n",
    "    check_val_every_n_epoch=30,\n",
    "    callbacks = [checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamodule import UnivariateTestDataset\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "train_set = UnivariateTestDataset(val_path)\n",
    "embs = model.encoder(torch.Tensor(train_set.time_series[:,None,:])).detach().numpy()\n",
    "labels = train_set.labels\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=21)\n",
    "projected_emb = pd.DataFrame(np.concatenate((labels[:,None], tsne.fit_transform(embs)), axis=1), columns = ['labels', 'x', 'y'])\n",
    "projected_emb['labels'] = projected_emb['labels'].astype('int').astype('str')\n",
    "fig = px.scatter(projected_emb, x = 'x', y='y', color='labels')\n",
    "fig.write_html(\"./test_tsne_FordA_without_Train.html\", auto_play=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Run algorithms on all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "\n",
    "\n",
    "dataset_name = []\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "for dir_path in glob(os.path.join(root_data,'UCR','*')):\n",
    "    \n",
    "    print('Doing {}'.format(os.path.basename(dir_path)))\n",
    "    train_path = glob(os.path.join(dir_path,'*TRAIN.tsv'))[0] if len(glob(os.path.join(dir_path,'*TRAIN.tsv')))>0 else None\n",
    "    test_path = glob(os.path.join(dir_path,'*TEST.tsv')) if len(glob(os.path.join(dir_path,'*TEST.tsv')))>0 else None\n",
    "    \n",
    "    if train_path is None or test_path is None or os.path.basename(dir_path)=='PigAirwayPressure': \n",
    "        continue\n",
    "    \n",
    "\n",
    "    # Datamodule importation\n",
    "    datamodule = TimeSeriesDataModule(train_path, val_path, batch_size, num_workers, min_length=20, multivariate=False, fill_na=True)\n",
    "\n",
    "    # Model definition\n",
    "    model = TimeSeriesEmbedder(\n",
    "        in_channels = in_channels,\n",
    "        channels = channels,\n",
    "        depth = depth,\n",
    "        reduced_size = reduced_size,\n",
    "        out_channels = out_channels,\n",
    "        kernel_size = kernel_size,\n",
    "        lr=lr,\n",
    "        weight_decay = weight_decay,\n",
    "        betas = betas,\n",
    "        train_path=train_path,\n",
    "        test_path=val_path\n",
    "    )\n",
    "    \n",
    "    epochs = 200\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "         mode ='min',\n",
    "         monitor='train_loss_epoch',\n",
    "         dirpath='checkpoints',\n",
    "        filename='causalcnn-{epoch:02d}-{train_loss_epoch:.2f}')\n",
    "\n",
    "    wandb_logger = pl.loggers.WandbLogger(project='Self Supervised Time Series Learning', name='ALL EXPERIMENTS - {}'.format(os.path.basename(dir_path)), prefix=os.path.basename(dir_path))\n",
    "    trainer  = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        logger = wandb_logger,\n",
    "        check_val_every_n_epoch=30,\n",
    "        callbacks = [checkpoint_callback])\n",
    "    trainer.fit(model, datamodule)\n",
    "    fig = trainer.test()\n",
    "    \n",
    "    try:\n",
    "        train_score, test_score = model.compute_scores()\n",
    "        print('\\n', train_score, test_score)\n",
    "        dataset_name.append(os.path.basename(dir_path))\n",
    "        train_score_list.append(train_score)\n",
    "        test_score_list.append(test_score)\n",
    "    except:\n",
    "        print('Dataset: {} does not have enough example per class (will fix that)'.format(os.path.basename(dir_path)))\n",
    "    \n",
    "results = pd.DataFrame(np.array([dataset_name,train_score_list, test_score_list]).transpose(), columns=['Name', 'Train Accuracy', 'Test Accuracy'])\n",
    "results.to_csv('./results_univariate_without_training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Multivariate time series - BasicMotions\n",
    "\n",
    "This section aims at studying the article in the context of multivariate time series. As an exploratory example, we used the dataset **Basic Motions**:\n",
    "    \n",
    "        \"The data was generated as part of a student project where four students performed four activities whilst wearing a smart watch. The watch collects 3D accelerometer and a 3D gyroscope It consists of four classes, which are walking, resting, running and badminton. Participants were required to record motion a total of five times, and the data is sampled once every tenth of a second, for a ten second period.\"\n",
    "\n",
    "This section contains:\n",
    "* A visualization of the dataset\n",
    "* Some experiments on the model provided by the author, on this multivariate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importation of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "from pyts.datasets import load_basic_motions,uea_dataset_list\n",
    "X_train, X_test, y_train, y_test = load_basic_motions(return_X_y=True)\n",
    "uea_dataset_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: normalization\n",
    "X_train = (X_train - X_train.mean(axis=2)[:,:,None])/X_train.std(axis=2)[:,:,None]\n",
    "X_test = (X_test - X_test.mean(axis=2)[:,:,None])/X_test.std(axis=2)[:,:,None]\n",
    "\n",
    "\n",
    "X_train = torch.from_numpy(X_train).double()\n",
    "X_test = torch.from_numpy(X_test).double()\n",
    "if torch.cuda.is_available():\n",
    "    X_train = X_train.cuda()\n",
    "    X_test = X_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "    The data was generated as part of a student project where four students performed four activities whilst wearing a smart watch. The watch collects 3D accelerometer and a 3D gyroscope It consists of four classes, which are walking, resting, running and badminton. Participants were required to record motion a total of five times, and the data is sampled once every tenth of a second, for a ten second period.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Accelerometer - X\", \"Accelerometer - Y\", \"Accelerometer - Z\", \"Gyroscope - Yaw\", \"Gyroscope - Pitch\", \"Gyroscope - Roll\"]\n",
    "if True:\n",
    "    index = 0\n",
    "    fig = plt.figure(figsize=(30,15))\n",
    "    axes = []\n",
    "    for i in range(6):\n",
    "        axes.append(fig.add_subplot(6,2,2*i+1))\n",
    "        axes[i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)\n",
    "        axes[i].plot(X_train[index,i,:], label = labels[i])\n",
    "        axes[i].legend(loc=1)\n",
    "    index = -1\n",
    "    for i in range(6):\n",
    "        axes.append(fig.add_subplot(6,2,2*(i+1)))\n",
    "        axes[6+i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)\n",
    "        axes[6+i].plot(X_train[index,i,:], label = labels[i])\n",
    "        axes[6+i].legend(loc=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "## Causal CNN Encoder - Exploration\n",
    "\n",
    "This section runs different experiment to investigate the role of the different module of the architecture:\n",
    "\n",
    "* The global average pooling layer to squeeze the temporal dimension, which is supposed to regularize the model compared to using a fully-connected layer.\n",
    "* The different hyper-parameters of the model:\n",
    "        * The depth of the network (i.e the number of causal CNN blocks, e.g 10 by default)\n",
    "        * The number of channels (e.g 40 by default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "###  Global Average Pooling as a Regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 6\n",
    "channels = 40\n",
    "depth = 4\n",
    "reduced_size = 160\n",
    "out_channels = 320\n",
    "kernel_size = 3\n",
    "N_sample = 100\n",
    "\n",
    "# The whole model\n",
    "causalEncoder = CausalCNNEncoder(in_channels = in_channels, channels = channels, depth = depth,\n",
    "                                 reduced_size = reduced_size, out_channels = out_channels, kernel_size = kernel_size).double()\n",
    "\n",
    "# The whole model without the last global average pooling and FC between reduced_size and out_channel\n",
    "causal_cnn = CausalCNN(in_channels = in_channels, channels = channels, depth = depth, out_channels = out_channels, kernel_size = kernel_size).double()\n",
    "\n",
    "\n",
    "\n",
    "##### BUILDING EACH BLOCK OF THE MODEL\n",
    "\n",
    "# Each of the Causal CNN Blocks (of width 4)\n",
    "ConvBlock_1 = CausalConvolutionBlock(in_channels = in_channels, out_channels = channels, dilation=1, kernel_size=3).double()\n",
    "ConvBlock_2 = CausalConvolutionBlock(in_channels = channels, out_channels = channels, dilation=2, kernel_size=3).double()\n",
    "ConvBlock_3 = CausalConvolutionBlock(in_channels = channels, out_channels = channels, dilation=4, kernel_size=3).double()\n",
    "ConvBlock_4 = CausalConvolutionBlock(in_channels = channels, out_channels = reduced_size , dilation=8, kernel_size=3).double()\n",
    "\n",
    "# Global average pooling\n",
    "reduce_size = torch.nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "#Squeez the last (third) temporal dimension \n",
    "squeeze = SqueezeChannels() \n",
    "\n",
    "# last fully connected layer to go from reduced_size to out_channel\n",
    "linear = torch.nn.Linear(reduced_size, out_channels).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data shape flow using the global average pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_scratch = [ConvBlock_1, ConvBlock_2, ConvBlock_3, ConvBlock_4, reduce_size, squeeze, linear]\n",
    "model_name_from_scratch = ['ConvBlock_1', 'ConvBlock_2', 'ConvBlock_3', 'ConvBlock_4', 'Global Average Pooling', 'squeezing', 'final FC']\n",
    "\n",
    "print('Input Shape:')\n",
    "print(list(X_train.shape),'\\n')\n",
    "input = X_train\n",
    "for block_ii in range(len(model_from_scratch)):\n",
    "    print('{}:'.format(model_name_from_scratch[block_ii]))\n",
    "    output = model_from_scratch[block_ii](input)\n",
    "    print(list(output.shape),'\\n')\n",
    "    input = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data shape flow using the a fully-connected layer to squeeze the temporal dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of the Global Average Pooling\n",
    "linear_to_squeeze = torch.nn.Linear(N_sample, 1).double()\n",
    "\n",
    "\n",
    "model_from_scratch_experiment = [ConvBlock_1, ConvBlock_2, ConvBlock_3, ConvBlock_4, linear_to_squeeze, squeeze, linear]\n",
    "model_name_from_scratch_experiment = ['ConvBlock_1', 'ConvBlock_2', 'ConvBlock_3', 'ConvBlock_4', 'linear_to_squeeze', 'squeezing', 'final FC']\n",
    "\n",
    "print('Input Shape:')\n",
    "print(list(X_train.shape),'\\n')\n",
    "input = X_train\n",
    "for block_ii in range(len(model_from_scratch_experiment)):\n",
    "    print('{}:'.format(model_name_from_scratch_experiment[block_ii]))\n",
    "    output = model_from_scratch_experiment[block_ii](input)\n",
    "    print(list(output.shape),'\\n')\n",
    "    input = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of the performances**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "### Analysis of the hyperparameters of the model\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building the multivariate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters \n",
    "batch_size = 3\n",
    "num_workers = 4\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 1e-2\n",
    "lr = 0.001\n",
    "\n",
    "# Model parameter \n",
    "in_channels = 1\n",
    "channels = 40\n",
    "depth = 4\n",
    "reduced_size = 160\n",
    "out_channels = 320\n",
    "kernel_size = 3\n",
    "N_sample = 288\n",
    "\n",
    "# Data parameters\n",
    "train_path = os.path.join(root_data,'FordA','FordA_TRAIN.tsv')\n",
    "val_path = os.path.join(root_data,'FordA','FordA_TEST.tsv')\n",
    "dataset_name='ArticularyWordRecognition'\n",
    "root_data_multivariate='./data/Multivariate'\n",
    "\n",
    "# Datamodule importation\n",
    "datamodule = TimeSeriesDataModule('', '', batch_size, num_workers, dataset_name, root_data_multivariate, min_length=20, multivariate=True, fill_na=True)\n",
    "\n",
    "# Model definition\n",
    "model = TimeSeriesEmbedder(\n",
    "    in_channels = in_channels,\n",
    "    channels = channels,\n",
    "    depth = depth,\n",
    "    reduced_size = reduced_size,\n",
    "    out_channels = out_channels,\n",
    "    kernel_size = kernel_size,\n",
    "    lr=lr,\n",
    "    weight_decay = weight_decay,\n",
    "    betas = betas,\n",
    "    train_path=train_path,\n",
    "    test_path=val_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = datamodule.train_dataloader()\n",
    "test_dataloader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.dataset[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TRAIN.tsv')\n",
    "dataset_univariate = UnivariateTrainDataset(train_path, fill_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "from pyts.datasets import load_basic_motions,uea_dataset_list, fetch_uea_dataset\n",
    "X_train, X_test, y_train, y_test = fetch_uea_dataset('BasicMotions', use_cache=False, data_home='./data/Multivariate/', return_X_y=True)\n",
    "time_series = X_train\n",
    "idx=0\n",
    "min_length = 20\n",
    "entire_series = time_series[idx]\n",
    "entire_length = entire_series.shape[1]\n",
    "pos_length = np.random.randint(min_length, high = entire_length+1)\n",
    "ref_length = np.random.randint(pos_length, high = entire_length+1)\n",
    "ref_beg = np.random.randint(0, high = entire_length+1-ref_length)\n",
    "pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)\n",
    "\n",
    "ref_series = entire_series[:,ref_beg:ref_beg+ref_length]\n",
    "pos_series = entire_series[:,pos_beg:pos_beg+pos_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.datasets import fetch_uea_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class MultivariateTrainDataset(Dataset):\n",
    "    '''\n",
    "    Dataset for the Univariate Time series training case. \n",
    "    This dataset sample a reference time series and a positive example\n",
    "    Parameters:\n",
    "    -----------\n",
    "        path: str,\n",
    "            Path to the data file. The time series must be in a tsv format. \n",
    "            The fist column correspond to labels and the rest to the time series\n",
    "        min_length: int, Optional, default=20\n",
    "            Minimum size of the subsample time series \n",
    "    '''\n",
    "    def __init__(self, dataset, root_data='./data/Multivariate/', min_length=20, fill_na=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_series, _ , _, _ = fetch_uea_dataset(dataset, use_cache=False, data_home=root_data, return_X_y=True)\n",
    "        \n",
    "        if fill_na:\n",
    "            #print('Percentage of Nan in the training set: {:.2f}\\nRemoving nan...'.format(100*np.isnan(self.time_series).sum()/(self.time_series.shape[0]*self.time_series.shape[1])))\n",
    "            nan_mask = np.isnan(self.time_series)\n",
    "            self.time_series[nan_mask] = np.zeros(shape=np.count_nonzero(nan_mask))\n",
    "        self.min_length = min_length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        entire_series = self.time_series[idx]\n",
    "        entire_length = entire_series.shape[1]\n",
    "        \n",
    "        pos_length = np.random.randint(self.min_length, high = entire_length+1)\n",
    "        \n",
    "        ref_length = np.random.randint(pos_length, high = entire_length+1)\n",
    "        \n",
    "        ref_beg = np.random.randint(0, high = entire_length+1-ref_length)\n",
    "        \n",
    "        pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)\n",
    "        \n",
    "        ref_series = entire_series[:,ref_beg:ref_beg+ref_length]\n",
    "        \n",
    "        pos_series = entire_series[:,pos_beg:pos_beg+pos_length]\n",
    "        \n",
    "        return torch.FloatTensor(ref_series), torch.FloatTensor(pos_series)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.time_series.shape[1]\n",
    "    \n",
    "def multivariate_train_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function to create the batch for training. Used to pad the time-series so they fit in the same batch.\n",
    "    Auxiliary function for the dataloader\n",
    "    Parameters:\n",
    "    -----------\n",
    "        batch: list\n",
    "            List containing the individual dataset items.\n",
    "    Output:\n",
    "    -------\n",
    "        padded_ref_series: Torch Tensor (batch_size,max_ref_series_length)\n",
    "            Padded reference series\n",
    "        padded_ref_series: Torch Tensor (batch_size,max_pos_series_length)\n",
    "            Padded positives series\n",
    "    \"\"\"\n",
    "    ref_series_list = list()\n",
    "    pos_series_list = list()\n",
    "    for item in batch:\n",
    "        ref_series_list.append(item[0].t())\n",
    "        pos_series_list.append(item[1].t())\n",
    "    return (\n",
    "        ref_series_list,\n",
    "        pad_sequence(ref_series_list, batch_first=True, padding_value=0.0).permute(0,2,1),\n",
    "        pad_sequence(pos_series_list, batch_first=True, padding_value=0.0).permute(0,2,1),\n",
    "    )\n",
    "\n",
    "\n",
    "train_set = MultivariateTrainDataset('BasicMotions', root_data='./data/Multivariate/', min_length=20, fill_na=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_set,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=multivariate_train_collate_fn,\n",
    "                    batch_size=3,#self.batch_size,\n",
    "                    num_workers=2)#self.num_workers)\n",
    "\n",
    "batch = [train_set[0], train_set[1], train_set[2]]\n",
    "ref_series_list,ref_series, pos_series = multivariate_train_collate_fn(batch)\n",
    "\n",
    "a = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_set[0], train_set[1], train_set[2]]\n",
    "ref_series_list,ref_series, pos_series = multivariate_train_collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_series_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.datasets import fetch_uea_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class UnivariateTrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the Univariate Time series training case. \n",
    "    This dataset sample a reference time series and a positive example\n",
    "    Parameters:\n",
    "    -----------\n",
    "        path: str,\n",
    "            Path to the data file. The time series must be in a tsv format. \n",
    "            The fist column correspond to labels and the rest to the time series\n",
    "        min_length: int, Optional, default=20\n",
    "            Minimum size of the subsample time series \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, min_length=20, fill_na=False):\n",
    "        super().__init__()\n",
    "        data = pd.read_csv(path, sep=\"\\t\", header=None)\n",
    "        self.time_series = np.array(data.iloc[:, 1:])\n",
    "\n",
    "        if fill_na:\n",
    "            # print('Percentage of Nan in the training set: {:.2f}\\nRemoving nan...'.format(100*np.isnan(self.time_series).sum()/(self.time_series.shape[0]*self.time_series.shape[1])))\n",
    "            nan_mask = np.isnan(self.time_series)\n",
    "            self.time_series[nan_mask] = np.zeros(shape=np.count_nonzero(nan_mask))\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entire_series = self.time_series[idx]\n",
    "        entire_length = entire_series.shape[0]\n",
    "\n",
    "        pos_length = np.random.randint(self.min_length, high=entire_length + 1)\n",
    "\n",
    "        ref_length = np.random.randint(pos_length, high=entire_length + 1)\n",
    "\n",
    "        ref_beg = np.random.randint(0, high=entire_length + 1 - ref_length)\n",
    "\n",
    "        pos_beg = np.random.randint(ref_beg, high=ref_beg + ref_length - pos_length + 1)\n",
    "\n",
    "        ref_series = entire_series[ref_beg : ref_beg + ref_length]\n",
    "\n",
    "        pos_series = entire_series[pos_beg : pos_beg + pos_length]\n",
    "\n",
    "        return torch.FloatTensor(ref_series), torch.FloatTensor(pos_series)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.time_series.shape[0]\n",
    "    \n",
    "\n",
    "def univariate_train_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function to create the batch for training. Used to pad the time-series so they fit in the same batch.\n",
    "    Auxiliary function for the dataloader\n",
    "    Parameters:\n",
    "    -----------\n",
    "        batch: list\n",
    "            List containing the individual dataset items.\n",
    "    Output:\n",
    "    -------\n",
    "        padded_ref_series: Torch Tensor (batch_size,max_ref_series_length)\n",
    "            Padded reference series\n",
    "        padded_ref_series: Torch Tensor (batch_size,max_pos_series_length)\n",
    "            Padded positives series\n",
    "    \"\"\"\n",
    "    ref_series_list = list()\n",
    "    pos_series_list = list()\n",
    "    for item in batch:\n",
    "        ref_series_list.append(item[0])\n",
    "        pos_series_list.append(item[1])\n",
    "    return (\n",
    "        pad_sequence(ref_series_list, batch_first=True, padding_value=0.0),\n",
    "        pad_sequence(pos_series_list, batch_first=True, padding_value=0.0),\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "root_data = './data'\n",
    "train_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TRAIN.tsv')\n",
    "\n",
    "train_set = UnivariateTrainDataset(train_path, min_length=20, fill_na=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_set,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=univariate_train_collate_fn,\n",
    "                    batch_size=3)\n",
    "\n",
    "batch = [train_set[0], train_set[1], train_set[2]]\n",
    "ref_series, pos_series = univariate_train_collate_fn(batch)\n",
    "\n",
    "a = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_set[0], train_set[1], train_set[2]]\n",
    "ref_series, pos_series = univariate_train_collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_series_list[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [train_set[0], train_set[1], train_set[2]]\n",
    "ref_series, pos_series = multivariate_train_collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_series, pos_series = multivariate_train_collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_series_list = list()\n",
    "pos_series_list = list()\n",
    "\n",
    "for item in batch:\n",
    "    print(item[0].shape)\n",
    "    ref_series_list.append(item[0].t())\n",
    "    pos_series_list.append(item[1].t())\n",
    "pad_sequence(ref_series_list, batch_first=True, padding_value=0.0).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item[0].t().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_series_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization for the report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_UCR_dataset(root_data,'DodgerLoopDay')\n",
    "\n",
    "print(\"X_train: {}\".format(X_train.shape))\n",
    "print(\"y_train: {}\".format(y_train.shape))\n",
    "print(\"X_test: {}\".format(X_test.shape))\n",
    "print(\"y_test: {}\".format(y_test.shape))\n",
    "\n",
    "print('Percentage of Nan in the training set: {}'.format(100*np.isnan(X_train).sum()/(X_train.shape[0]*X_train.shape[2])))\n",
    "fig,(ax1,ax2) = plt.subplots(2,1, figsize=(20,8))\n",
    "fig.suptitle('Time serie before and after replaing NaN Value', weight='bold', fontsize=18)\n",
    "ax1.plot(X_train[16,0,:], label='Original time serie')\n",
    "nan_mask = np.isnan(X_train)\n",
    "X_train[nan_mask] = np.zeros(shape=np.count_nonzero(nan_mask))\n",
    "ax2.plot(X_train[16,0,:], label='Time serie transformed')\n",
    "ax1.legend()\n",
    "_ = ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TRAIN.tsv')\n",
    "fill_na = True\n",
    "min_length = 20\n",
    "\n",
    "data = pd.read_csv(train_path,sep='\\t', header=None)\n",
    "time_series = np.array(data.iloc[:,1:])\n",
    "\n",
    "if fill_na:\n",
    "    #print('Percentage of Nan in the training set: {:.2f}\\nRemoving nan...'.format(100*np.isnan(self.time_series).sum()/(self.time_series.shape[0]*self.time_series.shape[1])))\n",
    "    nan_mask = np.isnan(time_series)\n",
    "    time_series[nan_mask] = np.zeros(shape=np.count_nonzero(nan_mask))\n",
    "\n",
    "    \n",
    "idx=0\n",
    "\n",
    "entire_series = time_series[idx]\n",
    "entire_length = entire_series.shape[0]\n",
    "pos_length = np.random.randint(min_length, high = entire_length+1)\n",
    "ref_length = np.random.randint(pos_length, high = entire_length+1)\n",
    "ref_beg = np.random.randint(0, high = entire_length+1-ref_length)\n",
    "pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)\n",
    "ref_series = entire_series[ref_beg:ref_beg+ref_length]\n",
    "pos_series = entire_series[pos_beg:pos_beg+pos_length]\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(30,8))\n",
    "plt.title('Example time serie, with reference and positive samples', weight='bold', fontsize=20)\n",
    "plt.plot(entire_series, color='k', label='Original Time Series')\n",
    "plt.plot(np.arange(ref_beg,ref_beg+ref_length),entire_series[ref_beg:ref_beg+ref_length], color='b', label='Ref Time Series')\n",
    "plt.plot(np.arange(pos_beg,pos_beg+pos_length),entire_series[pos_beg:pos_beg+pos_length], color='r', label='Pos Time Series')\n",
    "plt.xlabel('Time')\n",
    "_ = plt.legend(prop={'size':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.datasets import load_basic_motions,uea_dataset_list, fetch_uea_dataset\n",
    "X_train, X_test, y_train, y_test = fetch_uea_dataset('BasicMotions', use_cache=False, data_home='./data/Multivariate/', return_X_y=True)\n",
    "labels = [\"Accelerometer - X\", \"Accelerometer - Y\", \"Accelerometer - Z\", \"Gyroscope - Yaw\", \"Gyroscope - Pitch\", \"Gyroscope - Roll\"]\n",
    "if True:\n",
    "    index = 0\n",
    "    fig = plt.figure(figsize=(30,15))\n",
    "    axes = []\n",
    "    for i in range(6):\n",
    "        axes.append(fig.add_subplot(6,2,2*i+1))\n",
    "        axes[i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)\n",
    "        axes[i].plot(X_train[index,i,:], label = labels[i])\n",
    "        axes[i].legend(loc=1)\n",
    "    index = -1\n",
    "    for i in range(6):\n",
    "        axes.append(fig.add_subplot(6,2,2*(i+1)))\n",
    "        axes[6+i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)\n",
    "        axes[6+i].plot(X_train[index,i,:], label = labels[i])\n",
    "        axes[6+i].legend(loc=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "time_series = X_train\n",
    "\n",
    "\n",
    "idx=0\n",
    "min_length = 20\n",
    "entire_series = time_series[idx]\n",
    "entire_length = entire_series.shape[1]\n",
    "pos_length = np.random.randint(min_length, high = entire_length+1)\n",
    "ref_length = np.random.randint(pos_length, high = entire_length+1)\n",
    "ref_beg = np.random.randint(0, high = entire_length+1-ref_length)\n",
    "pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)\n",
    "\n",
    "ref_series = entire_series[:,ref_beg:ref_beg+ref_length]\n",
    "pos_series = entire_series[:,pos_beg:pos_beg+pos_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Accelerometer - X\", \"Accelerometer - Y\", \"Accelerometer - Z\", \"Gyroscope - Yaw\", \"Gyroscope - Pitch\", \"Gyroscope - Roll\"]\n",
    "if True:\n",
    "    index = 0\n",
    "    min_length = 20\n",
    "    entire_series = time_series[index]\n",
    "    entire_length = entire_series.shape[1]\n",
    "    pos_length = np.random.randint(min_length, high = entire_length+1)\n",
    "    ref_length = np.random.randint(pos_length, high = entire_length+1)\n",
    "    ref_beg = np.random.randint(0, high = entire_length+1-ref_length)\n",
    "    pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)\n",
    "\n",
    "    ref_series = entire_series[:,ref_beg:ref_beg+ref_length]\n",
    "    pos_series = entire_series[:,pos_beg:pos_beg+pos_length]\n",
    "\n",
    "    fig = plt.figure(figsize=(30,15))\n",
    "    axes = []\n",
    "    for i in range(6):\n",
    "        axes.append(fig.add_subplot(6,2,2*i+1))\n",
    "        axes[i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)\n",
    "        #axes[i].plot(X_train[index,i,:], label = labels[i])\n",
    "        axes[i].plot(entire_series[i,:], color='k', label='Entire Serie')\n",
    "        axes[i].plot(np.arange(ref_beg,ref_beg+ref_length),entire_series[i,ref_beg:ref_beg+ref_length], color='b', label='Ref Time Series')\n",
    "        axes[i].plot(np.arange(pos_beg,pos_beg+pos_length),entire_series[i,pos_beg:pos_beg+pos_length], color='r', label='Pos Time Series')\n",
    "        axes[i].legend(loc=1)\n",
    "        \n",
    "    index = -1\n",
    "    min_length = 20\n",
    "    entire_series = time_series[index]\n",
    "    entire_length = entire_series.shape[1]\n",
    "    pos_length = np.random.randint(min_length, high = entire_length+1)\n",
    "    ref_length = np.random.randint(pos_length, high = entire_length+1)\n",
    "    ref_beg = np.random.randint(0, high = entire_length+1-ref_length)\n",
    "    pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)\n",
    "    ref_series = entire_series[:,ref_beg:ref_beg+ref_length]\n",
    "    pos_series = entire_series[:,pos_beg:pos_beg+pos_length]\n",
    "\n",
    "    for i in range(6):\n",
    "        axes.append(fig.add_subplot(6,2,2*(i+1)))\n",
    "        axes[6+i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)\n",
    "        #axes[6+i].plot(X_train[index,i,:], label = labels[i])\n",
    "        axes[6+i].plot(entire_series[i,:], color='k', label='Entire Serie')\n",
    "        axes[6+i].plot(np.arange(ref_beg,ref_beg+ref_length),entire_series[i,ref_beg:ref_beg+ref_length], color='b', label='Ref Time Series')\n",
    "        axes[6+i].plot(np.arange(pos_beg,pos_beg+pos_length),entire_series[i,pos_beg:pos_beg+pos_length], color='r', label='Pos Time Series')\n",
    "        axes[6+i].legend(loc=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python37564bitbasecondaa50c3607aaec4d41a1290a133b998fce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
